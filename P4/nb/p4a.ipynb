{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32417ccf-7f7b-427f-aaf1-0b000da56eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs\n",
    "import requests\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c9ec04-5b65-4d87-8537-0633c9987437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 49766334464 (46.35 GB)\n",
      "Present Capacity: 16162221180 (15.05 GB)\n",
      "DFS Remaining: 15633235968 (14.56 GB)\n",
      "DFS Used: 528985212 (504.48 MB)\n",
      "DFS Used%: 3.27%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 0\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 0\n",
      "\tMissing blocks (with replication factor 1): 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (2):\n",
      "\n",
      "Name: 172.20.0.3:9866 (p4_wadadkar-dn-1.p4_wadadkar_default)\n",
      "Hostname: 0190684eeb36\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 24883167232 (23.17 GB)\n",
      "DFS Used: 261765966 (249.64 MB)\n",
      "Non DFS Used: 16788006066 (15.64 GB)\n",
      "DFS Remaining: 7816617984 (7.28 GB)\n",
      "DFS Used%: 1.05%\n",
      "DFS Remaining%: 31.41%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Fri Nov 15 06:40:47 GMT 2024\n",
      "Last Block Report: Fri Nov 15 06:37:30 GMT 2024\n",
      "Num of Blocks: 248\n",
      "\n",
      "\n",
      "Name: 172.20.0.5:9866 (p4_wadadkar-dn-2.p4_wadadkar_default)\n",
      "Hostname: d529d9fd4a2d\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 24883167232 (23.17 GB)\n",
      "DFS Used: 267219246 (254.84 MB)\n",
      "Non DFS Used: 16782552786 (15.63 GB)\n",
      "DFS Remaining: 7816617984 (7.28 GB)\n",
      "DFS Used%: 1.07%\n",
      "DFS Remaining%: 31.41%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Fri Nov 15 06:40:48 GMT 2024\n",
      "Last Block Report: Fri Nov 15 06:37:30 GMT 2024\n",
      "Num of Blocks: 253\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q1\n",
    "!hdfs dfsadmin -fs hdfs://boss:9000 -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7472756e-b0de-4372-b49c-3dd59949630d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "# Check if the file already exists and download it if not\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"/nb/hdma-wi-2021.csv\"\n",
    "url = \"https://pages.cs.wisc.edu/~harter/cs544/data/hdma-wi-2021.csv\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(\"File not found locally. Downloading...\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"File already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb778223-8faf-4b04-8f45-10f976107b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hdfs://boss:9000/single.csv\n",
      "Deleted hdfs://boss:9000/double.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -f hdfs://boss:9000/single.csv\n",
    "!hdfs dfs -rm -f hdfs://boss:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a147211-4e6d-4fb0-ac61-8a73f829378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the HDFS block size to 1MB and upload the file to two different locations with different replication factors\n",
    "!hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=1 -put /nb/hdma-wi-2021.csv hdfs://boss:9000/single.csv\n",
    "!hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=2 -put /nb/hdma-wi-2021.csv hdfs://boss:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d26b72e-50de-46a0-bdea-6847958ae0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.8 M  166.8 M  hdfs://boss:9000/single.csv\n",
      "166.8 M  333.7 M  hdfs://boss:9000/double.csv\n"
     ]
    }
   ],
   "source": [
    "#q2\n",
    "# Check the logical and physical sizes of the CSV files in HDFS\n",
    "!hdfs dfs -du -h hdfs://boss:9000/single.csv\n",
    "!hdfs dfs -du -h hdfs://boss:9000/double.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fb80781-64f8-45db-8386-a5792325cab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FileStatus': {'accessTime': 1731652859388,\n",
       "  'blockSize': 1048576,\n",
       "  'childrenNum': 0,\n",
       "  'fileId': 16388,\n",
       "  'group': 'supergroup',\n",
       "  'length': 174944099,\n",
       "  'modificationTime': 1731652863818,\n",
       "  'owner': 'root',\n",
       "  'pathSuffix': '',\n",
       "  'permission': '644',\n",
       "  'replication': 1,\n",
       "  'storagePolicy': 0,\n",
       "  'type': 'FILE'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "file_url = \"http://boss:9870/webhdfs/v1/single.csv\"\n",
    "params = {\"op\": \"GETFILESTATUS\"}\n",
    "response=requests.get(file_url, params=params)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ef15596-27a2-4256-bb4c-24262524918f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://0190684eeb36:9864/webhdfs/v1/single.csv?op=OPEN&namenoderpcaddress=boss:9000&offset=0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "# Define the WebHDFS URL with noredirect=true\n",
    "file_url = \"http://boss:9870/webhdfs/v1/single.csv\"  # Changed from single.parquet to single.csv\n",
    "params = {\n",
    "    \"op\": \"OPEN\",\n",
    "    \"offset\": 0,\n",
    "    \"noredirect\": \"true\"\n",
    "}\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(file_url, params=params)\n",
    "location = response.json().get(\"Location\")\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14fa3d9a-a312-434a-85be-821c116c046e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0190684eeb36': 78, 'd529d9fd4a2d': 89}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "\n",
    "# Define the WebHDFS URL and parameters for the OPEN operation\n",
    "file_url = \"http://boss:9870/webhdfs/v1/single.csv\"\n",
    "params = {\n",
    "    \"op\": \"GETFILESTATUS\"\n",
    "}\n",
    "# Step 1: Get file size and block size\n",
    "file_status = requests.get(file_url, params=params).json()\n",
    "file_size = file_status[\"FileStatus\"][\"length\"]\n",
    "block_size = file_status[\"FileStatus\"][\"blockSize\"]\n",
    "num_blocks = (file_size + block_size - 1) // block_size  # Calculate total number of blocks\n",
    "\n",
    "# Dictionary to store the block distribution by DataNode container ID\n",
    "block_distribution = {}\n",
    "# Step 2: Loop through each block to check its DataNode location\n",
    "for block_num in range(num_blocks):\n",
    "    # Set the offset for each block\n",
    "    params = {\n",
    "        \"op\": \"OPEN\",\n",
    "        \"offset\": block_num * block_size,\n",
    "        \"noredirect\": \"true\"\n",
    "    }\n",
    "    \n",
    "    # Request block location\n",
    "    response = requests.get(file_url, params=params)\n",
    "    location = response.json().get(\"Location\")\n",
    "    \n",
    "    # If location is found, extract the container ID\n",
    "    if location:\n",
    "        container_id = location.split(\"//\")[1].split(\":\")[0]\n",
    "    \n",
    "    if container_id in block_distribution:\n",
    "        block_distribution[container_id] += 1\n",
    "    else:\n",
    "        block_distribution[container_id] = 1\n",
    "block_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b25fe41e-e7e7-4fa9-8069-fece4dd6a005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 06:41:24,752 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'activity_y'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "# Connect to HDFS using PyArrow\n",
    "hdfs = pa.fs.HadoopFileSystem(\"boss\", 9000)  # Replace \"boss\" with your Namenode hostname and \"9000\" with the correct port\n",
    "\n",
    "# Define the path to the file on HDFS\n",
    "file_path = \"/single.csv\"\n",
    "\n",
    "# Open the file and read the first 10 bytes\n",
    "with hdfs.open_input_file(file_path) as file:\n",
    "    # Use read_at to read 10 bytes starting at offset 0\n",
    "    first_10_bytes = file.read_at(10, 0)\n",
    "\n",
    "first_10_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028b0f30-ec80-44b0-8e87-1b2746bcd873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444874"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "\n",
    "# Connect to HDFS on the Namenode at port 9000\n",
    "hdfs = pa.fs.HadoopFileSystem(\"boss\", 9000)\n",
    "\n",
    "# Open the file single.csv from HDFS\n",
    "file_path = \"/single.csv\"\n",
    "with hdfs.open_input_file(file_path) as file:\n",
    "    # Wrap the NativeFile object in BufferedReader and TextIOWrapper for line-by-line reading\n",
    "    buffer = io.BufferedReader(file)\n",
    "    text_file = io.TextIOWrapper(buffer)\n",
    "\n",
    "    # Count lines containing \"Single Family\"\n",
    "    count = sum(1 for line in text_file if \"Single Family\" in line)\n",
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

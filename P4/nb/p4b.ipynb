{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ffb6080-1293-4163-8ef1-2f5501601c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.fs\n",
    "import io\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df9ff9e-1d26-4e71-830c-6abead624f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 24883167232 (23.17 GB)\n",
      "Present Capacity: 8085766467 (7.53 GB)\n",
      "DFS Remaining: 7815376896 (7.28 GB)\n",
      "DFS Used: 270389571 (257.86 MB)\n",
      "DFS Used%: 3.34%\n",
      "Replicated Blocks:\n",
      "\tUnder replicated blocks: 167\n",
      "\tBlocks with corrupt replicas: 0\n",
      "\tMissing blocks: 78\n",
      "\tMissing blocks (with replication factor 1): 78\n",
      "\tLow redundancy blocks with highest priority to recover: 167\n",
      "\tPending deletion blocks: 0\n",
      "Erasure Coded Block Groups: \n",
      "\tLow redundancy block groups: 0\n",
      "\tBlock groups with corrupt internal blocks: 0\n",
      "\tMissing block groups: 0\n",
      "\tLow redundancy blocks with highest priority to recover: 0\n",
      "\tPending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 172.20.0.5:9866 (p4_wadadkar-dn-2.p4_wadadkar_default)\n",
      "Hostname: d529d9fd4a2d\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 24883167232 (23.17 GB)\n",
      "DFS Used: 270389571 (257.86 MB)\n",
      "Non DFS Used: 16780623549 (15.63 GB)\n",
      "DFS Remaining: 7815376896 (7.28 GB)\n",
      "DFS Used%: 1.09%\n",
      "DFS Remaining%: 31.41%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Fri Nov 15 06:44:12 GMT 2024\n",
      "Last Block Report: Fri Nov 15 06:37:30 GMT 2024\n",
      "Num of Blocks: 256\n",
      "\n",
      "\n",
      "Dead datanodes (1):\n",
      "\n",
      "Name: 172.20.0.3:9866 (172.20.0.3)\n",
      "Hostname: 0190684eeb36\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 24883167232 (23.17 GB)\n",
      "DFS Used: 258595641 (246.62 MB)\n",
      "Non DFS Used: 16792368327 (15.64 GB)\n",
      "DFS Remaining: 7815426048 (7.28 GB)\n",
      "DFS Used%: 1.04%\n",
      "DFS Remaining%: 31.41%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 0\n",
      "Last contact: Fri Nov 15 06:41:56 GMT 2024\n",
      "Last Block Report: Fri Nov 15 06:37:30 GMT 2024\n",
      "Num of Blocks: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#q8\n",
    "!hdfs dfsadmin -fs hdfs://boss:9000 -report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225a6c35-bb4c-4be5-82a7-c5be673d7eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f690b25-95c4-486f-9454-4a59013de06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lost': 78, 'd529d9fd4a2d': 89}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q9\n",
    "\n",
    "# Define the WebHDFS URL for accessing single.csv\n",
    "file_url = \"http://boss:9870/webhdfs/v1/single.csv\"\n",
    "params = {\n",
    "    \"op\": \"GETFILESTATUS\"\n",
    "}\n",
    "\n",
    "# Retrieve file size and block size\n",
    "file_status = requests.get(file_url, params=params).json()\n",
    "file_size = file_status[\"FileStatus\"][\"length\"]\n",
    "block_size = file_status[\"FileStatus\"][\"blockSize\"]\n",
    "num_blocks = (file_size + block_size - 1) // block_size  # Calculate the total number of blocks\n",
    "\n",
    "# Dictionary to store block distribution and a list for accessible blocks\n",
    "block_distribution = {\n",
    "    \"lost\": 0,\n",
    "}\n",
    "accessible_blocks = []\n",
    "\n",
    "# Loop through each block to check its DataNode location\n",
    "for block_num in range(num_blocks):\n",
    "    offset = block_num * block_size\n",
    "    params = {\n",
    "        \"op\": \"OPEN\",\n",
    "        \"offset\": offset,\n",
    "        \"noredirect\": \"true\"\n",
    "    }\n",
    "    \n",
    "    # Request block location\n",
    "    response = requests.get(file_url, params=params)\n",
    "    \n",
    "    # If the block is inaccessible, count it as \"lost\"\n",
    "    if response.status_code == 403:\n",
    "        block_distribution[\"lost\"] += 1\n",
    "    elif response.status_code == 200:\n",
    "        # Extract the container ID from the Location header if available\n",
    "        location = response.json().get(\"Location\")\n",
    "        container_id = location.split(\"//\")[1].split(\":\")[0]\n",
    "\n",
    "        if container_id in block_distribution:\n",
    "            block_distribution[container_id] += 1\n",
    "        else:\n",
    "            block_distribution[container_id] = 1\n",
    "        accessible_blocks.append(offset)\n",
    "block_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8390ec6a-8dd4-43c6-bce7-605539a6bb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-15 06:44:23,925 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "237366"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q10\n",
    "\n",
    "# Connect to HDFS on the Namenode at port 9000\n",
    "hdfs = pa.fs.HadoopFileSystem(\"boss\", 9000)\n",
    "file_path = \"/single.csv\"\n",
    "\n",
    "# Initialize count for occurrences of \"Single Family\"\n",
    "single_family_count = 0\n",
    "\n",
    "# Loop through each accessible block's offset\n",
    "for offset in accessible_blocks:\n",
    "    with hdfs.open_input_file(file_path) as file:\n",
    "        # Read only the accessible block's content\n",
    "        block_content = file.read_at(block_size, offset).decode('utf-8')\n",
    "\n",
    "        # Use io.StringIO to treat the block content as a text stream\n",
    "        text_stream = io.StringIO(block_content)\n",
    "        \n",
    "        # Count lines containing \"Single Family\" within this block\n",
    "        single_family_count += sum(1 for line in text_stream if \"Single Family\" in line)\n",
    "\n",
    "# Output the total count of \"Single Family\" in accessible blocks\n",
    "single_family_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b787b678-9009-4451-898b-ea4bc9094bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
